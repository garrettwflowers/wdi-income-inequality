{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416804b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import glob \n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Scipy stats for parameter distributions\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Disable file validation warning\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d9e54",
   "metadata": {},
   "source": [
    "# Target Selection and Configuration\n",
    "\n",
    "Choose the target variable for modeling from the options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2081117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target options for reference\n",
    "TARGETS = {\n",
    "    'GINI': 'SI.POV.GINI',      # Gini coefficient (measure of inequality)\n",
    "    'top10': 'SI.DST.10TH.10',  # Top 10% wealth share\n",
    "    'bottom10': 'SI.DST.FRST.10' # Bottom 10% wealth share\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960eeabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date ranges available:\n",
      "2022_to_2023\n",
      "1970_to_2020\n",
      "2000_to_2020\n",
      "1960_to_2024\n"
     ]
    }
   ],
   "source": [
    "print('Date ranges available:')\n",
    "for file in glob.glob('input/imputed/*.csv'):\n",
    "    date_start = file.split('/')[-1].split('.')[0].split('_')[-3]\n",
    "    date_end = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    print(f\"{date_start}_to_{date_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674de1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_target = 'bottom10'\n",
    "selected_date_range = '2000_to_2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e17d5",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Define all path variables here for easy modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15210abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFIGURATION SUMMARY ===\n",
      "Target Variable: bottom10 (SI.DST.FRST.10)\n",
      "Date Range: 2000_to_2020\n",
      "Random Seed: 20250406\n",
      "\n",
      "=== Input Files ===\n",
      "Data File: input/imputed/df_wide_knn_imputed_2000_to_2020.csv\n",
      "Indicator Lookup: output/reference/indicator_lookup.csv\n",
      "\n",
      "=== Output Files ===\n",
      "Base Model: output/models/xgb_base_model_bottom10_2000_to_2020.pkl\n",
      "Base Predictions: output/predictions/xgb_base_predictions_bottom10_2000_to_2020.csv\n",
      "Base Feature Importances: output/feature_importance/xgb_base_feature_importances_bottom10_2000_to_2020.csv\n",
      "Tuned Model: output/models/xgb_tuned_model_bottom10_2000_to_2020.pkl\n",
      "Tuned Predictions: output/predictions/xgb_tuned_predictions_bottom10_2000_to_2020.csv\n",
      "Tuned Feature Importances: output/feature_importance/xgb_tuned_feature_importances_bottom10_2000_to_2020.csv\n",
      "Tuned Model Parameters: output/models/xgb_tuned_model_params_bottom10_2000_to_2020.json\n",
      "\n",
      "=== Directories ===\n",
      "Input Directory: input\n",
      "Imputed Data Directory: input/imputed\n",
      "Transformed Data Directory: input/transformed\n",
      "Train/Test/Val Directory: input/train_test_val\n",
      "Reference Directory: output/reference\n",
      "Output Directory: output\n",
      "Models Directory: output/models\n",
      "Predictions Directory: output/predictions\n",
      "Feature Importance Directory: output/feature_importance\n"
     ]
    }
   ],
   "source": [
    "# Random seed for reproducibility\n",
    "SEED = 20250406\n",
    "\n",
    "# Target\n",
    "TARGET = TARGETS[selected_target]\n",
    "\n",
    "# Input paths\n",
    "INPUT_DIR = 'input'\n",
    "IMPUTED_DIR = f'{INPUT_DIR}/imputed'\n",
    "TRANSFORMED_DIR = f'{INPUT_DIR}/transformed'\n",
    "TRAIN_TEST_VAL_DIR = f'{INPUT_DIR}/train_test_val'\n",
    "REFERENCE_DIR = 'output/reference'\n",
    "\n",
    "# Data files\n",
    "DATA_FILE = f'{IMPUTED_DIR}/df_wide_knn_imputed_{selected_date_range}.csv'\n",
    "TARGET_FILE = f'{TRANSFORMED_DIR}/df_wide_targets_{selected_date_range}.csv'\n",
    "INDICATOR_LOOKUP_FILE = f'{REFERENCE_DIR}/indicator_lookup.csv'\n",
    "\n",
    "# Extract implied date range from data file name\n",
    "DATE_RANGE = re.search(r'(\\d+)_to_(\\d+)', DATA_FILE).group(0)\n",
    "\n",
    "# Set file suffix for output files based on the selected target\n",
    "FILE_SUFFIX = f\"{selected_target}_{DATE_RANGE}\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = 'output'\n",
    "MODELS_DIR = f'{OUTPUT_DIR}/models'\n",
    "PREDICTIONS_DIR = f'{OUTPUT_DIR}/predictions'\n",
    "FEATURE_IMPORTANCE_DIR = f'{OUTPUT_DIR}/feature_importance'\n",
    "\n",
    "# Output files\n",
    "# - Base model outputs\n",
    "MODEL_FILE = f'{MODELS_DIR}/xgb_base_model_{FILE_SUFFIX}.pkl'\n",
    "PREDICTIONS_FILE = f'{PREDICTIONS_DIR}/xgb_base_predictions_{FILE_SUFFIX}.csv'\n",
    "FEATURE_IMPORTANCES_FILE = f'{FEATURE_IMPORTANCE_DIR}/xgb_base_feature_importances_{FILE_SUFFIX}.csv'\n",
    "\n",
    "# - Best model outputs\n",
    "BEST_MODEL_FILE = f'{MODELS_DIR}/xgb_tuned_model_{FILE_SUFFIX}.pkl'\n",
    "BEST_PREDICTIONS_FILE = f'{PREDICTIONS_DIR}/xgb_tuned_predictions_{FILE_SUFFIX}.csv'\n",
    "BEST_FEATURE_IMPORTANCES_FILE = f'{FEATURE_IMPORTANCE_DIR}/xgb_tuned_feature_importances_{FILE_SUFFIX}.csv'\n",
    "BEST_PARAMS_FILE = f'{MODELS_DIR}/xgb_tuned_model_params_{FILE_SUFFIX}.json'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURE_IMPORTANCE_DIR, exist_ok=True)\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== CONFIGURATION SUMMARY ===\")\n",
    "print(f\"Target Variable: {selected_target} ({TARGET})\")\n",
    "print(f\"Date Range: {DATE_RANGE}\")\n",
    "print(f\"Random Seed: {SEED}\")\n",
    "print(\"\\n=== Input Files ===\")\n",
    "print(f\"Data File: {DATA_FILE}\")\n",
    "print(f\"Indicator Lookup: {INDICATOR_LOOKUP_FILE}\")\n",
    "print(\"\\n=== Output Files ===\")\n",
    "print(f\"Base Model: {MODEL_FILE}\")\n",
    "print(f\"Base Predictions: {PREDICTIONS_FILE}\")\n",
    "print(f\"Base Feature Importances: {FEATURE_IMPORTANCES_FILE}\")\n",
    "print(f\"Tuned Model: {BEST_MODEL_FILE}\")\n",
    "print(f\"Tuned Predictions: {BEST_PREDICTIONS_FILE}\")\n",
    "print(f\"Tuned Feature Importances: {BEST_FEATURE_IMPORTANCES_FILE}\")\n",
    "print(f\"Tuned Model Parameters: {BEST_PARAMS_FILE}\")\n",
    "print(\"\\n=== Directories ===\")\n",
    "print(f\"Input Directory: {INPUT_DIR}\")\n",
    "print(f\"Imputed Data Directory: {IMPUTED_DIR}\")\n",
    "print(f\"Transformed Data Directory: {TRANSFORMED_DIR}\")\n",
    "print(f\"Train/Test/Val Directory: {TRAIN_TEST_VAL_DIR}\")\n",
    "print(f\"Reference Directory: {REFERENCE_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Models Directory: {MODELS_DIR}\")\n",
    "print(f\"Predictions Directory: {PREDICTIONS_DIR}\")\n",
    "print(f\"Feature Importance Directory: {FEATURE_IMPORTANCE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5196303",
   "metadata": {},
   "source": [
    "# Select Data for Train/Test/Validation\n",
    "\n",
    "We'll use stratified sampling based on country to ensure each split has a representative distribution of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e29021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Splits ===\n",
      "Train Size: 2690 (69.98%)\n",
      "Validation Size: 577 (15.01%)\n",
      "Test Size: 577 (15.01%)\n",
      "\n",
      "=== Country Distribution Check ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Country Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Validation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Test",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Train %",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Val %",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test %",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c0597357-6a8a-400b-a589-c0e98042e4f5",
       "rows": [
        [
         "ABW",
         "15",
         "2",
         "3",
         "75.0",
         "10.0",
         "15.0"
        ],
        [
         "AFE",
         "10",
         "6",
         "4",
         "50.0",
         "30.0",
         "20.0"
        ],
        [
         "AFG",
         "14",
         "3",
         "3",
         "70.0",
         "15.0",
         "15.0"
        ],
        [
         "AFW",
         "14",
         "2",
         "4",
         "70.0",
         "10.0",
         "20.0"
        ],
        [
         "AGO",
         "10",
         "3",
         "4",
         "58.82352941176471",
         "17.647058823529413",
         "23.52941176470588"
        ],
        [
         "ALB",
         "4",
         "3",
         "3",
         "40.0",
         "30.0",
         "30.0"
        ],
        [
         "AND",
         "10",
         "6",
         "4",
         "50.0",
         "30.0",
         "20.0"
        ],
        [
         "ARB",
         "14",
         "1",
         "5",
         "70.0",
         "5.0",
         "25.0"
        ],
        [
         "ARE",
         "15",
         "3",
         "0",
         "83.33333333333334",
         "16.666666666666664",
         "0.0"
        ],
        [
         "ARG",
         "1",
         "0",
         "0",
         "100.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "      <th>Train %</th>\n",
       "      <th>Val %</th>\n",
       "      <th>Test %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABW</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFE</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFG</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFW</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGO</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>58.823529</td>\n",
       "      <td>17.647059</td>\n",
       "      <td>23.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALB</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARB</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARE</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARG</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Train  Validation  Test     Train %      Val %     Test %\n",
       "Country Code                                                           \n",
       "ABW              15           2     3   75.000000  10.000000  15.000000\n",
       "AFE              10           6     4   50.000000  30.000000  20.000000\n",
       "AFG              14           3     3   70.000000  15.000000  15.000000\n",
       "AFW              14           2     4   70.000000  10.000000  20.000000\n",
       "AGO              10           3     4   58.823529  17.647059  23.529412\n",
       "ALB               4           3     3   40.000000  30.000000  30.000000\n",
       "AND              10           6     4   50.000000  30.000000  20.000000\n",
       "ARB              14           1     5   70.000000   5.000000  25.000000\n",
       "ARE              15           3     0   83.333333  16.666667   0.000000\n",
       "ARG               1           0     0  100.000000   0.000000   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of countries with only one sample: 8\n",
      "Examples of countries with one sample: ['ARG', 'ARM', 'HND', 'IRL', 'ISR']\n"
     ]
    }
   ],
   "source": [
    "# Import additional modules for stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "# Load the data and targets\n",
    "data = pd.read_csv(DATA_FILE)\n",
    "targets = pd.read_csv(TARGET_FILE)\n",
    "feature_lookup = pd.read_csv(INDICATOR_LOOKUP_FILE)\n",
    "\n",
    "# Instead of stratifying by country, we'll just use a simple random split\n",
    "# This avoids the stratification issue with rare countries\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    data.index, \n",
    "    test_size=0.3, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Split the temp indices into validation and test sets\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Create DataFrames for the training, validation, and test sets\n",
    "train_data = data.iloc[train_idx].copy()\n",
    "val_data = data.iloc[val_idx].copy()\n",
    "test_data = data.iloc[test_idx].copy()\n",
    "\n",
    "# Create DataFrames for the targets\n",
    "train_targets = targets.iloc[train_idx].copy()\n",
    "val_targets = targets.iloc[val_idx].copy()\n",
    "test_targets = targets.iloc[test_idx].copy()\n",
    "\n",
    "# Save the train, validation, and test sets to CSV files\n",
    "train_data.to_csv(f'{TRAIN_TEST_VAL_DIR}/train_data_{FILE_SUFFIX}.csv', index=False)\n",
    "val_data.to_csv(f'{TRAIN_TEST_VAL_DIR}/val_data_{FILE_SUFFIX}.csv', index=False)\n",
    "test_data.to_csv(f'{TRAIN_TEST_VAL_DIR}/test_data_{FILE_SUFFIX}.csv', index=False)\n",
    "train_targets.to_csv(f'{TRAIN_TEST_VAL_DIR}/train_targets_{FILE_SUFFIX}.csv', index=False)\n",
    "val_targets.to_csv(f'{TRAIN_TEST_VAL_DIR}/val_targets_{FILE_SUFFIX}.csv', index=False)\n",
    "test_targets.to_csv(f'{TRAIN_TEST_VAL_DIR}/test_targets_{FILE_SUFFIX}.csv', index=False)\n",
    "\n",
    "# Print split information\n",
    "print(\"\\n=== Data Splits ===\")\n",
    "print(f\"Train Size: {len(train_data)} ({len(train_data) / len(data) * 100:.2f}%)\")\n",
    "print(f\"Validation Size: {len(val_data)} ({len(val_data) / len(data) * 100:.2f}%)\")\n",
    "print(f\"Test Size: {len(test_data)} ({len(test_data) / len(data) * 100:.2f}%)\")\n",
    "\n",
    "# Print country distribution in each split\n",
    "print(\"\\n=== Country Distribution Check ===\")\n",
    "train_countries = train_data['Country Code'].value_counts().sort_index()\n",
    "val_countries = val_data['Country Code'].value_counts().sort_index()\n",
    "test_countries = test_data['Country Code'].value_counts().sort_index()\n",
    "\n",
    "# Check a sample of countries to verify the distribution\n",
    "sample_countries = train_countries.index[:10]  # Take just the first 10 for display\n",
    "country_dist = pd.DataFrame({\n",
    "    'Train': [train_countries.get(c, 0) for c in sample_countries],\n",
    "    'Validation': [val_countries.get(c, 0) for c in sample_countries],\n",
    "    'Test': [test_countries.get(c, 0) for c in sample_countries]\n",
    "}, index=sample_countries)\n",
    "country_dist['Train %'] = country_dist['Train'] / (country_dist['Train'] + country_dist['Validation'] + country_dist['Test']) * 100\n",
    "country_dist['Val %'] = country_dist['Validation'] / (country_dist['Train'] + country_dist['Validation'] + country_dist['Test']) * 100\n",
    "country_dist['Test %'] = country_dist['Test'] / (country_dist['Train'] + country_dist['Validation'] + country_dist['Test']) * 100\n",
    "display(country_dist)\n",
    "\n",
    "# Count countries by frequency\n",
    "country_counts = Counter(data['Country Code'])\n",
    "single_occurrence_countries = [country for country, count in country_counts.items() if count == 1]\n",
    "print(f\"\\nNumber of countries with only one sample: {len(single_occurrence_countries)}\")\n",
    "if single_occurrence_countries:\n",
    "    print(f\"Examples of countries with one sample: {single_occurrence_countries[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183e8c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data dimensions after removing missing targets:\n",
      "Train Shape: 765 x 438\n",
      "Validation Shape: 161 x 438\n",
      "Test Shape: 150 x 438\n",
      "Train target elements: 765\n",
      "Validation target elements: 161\n",
      "Test target elements: 150\n",
      "Total observations: 1,076\n"
     ]
    }
   ],
   "source": [
    "# Select features and prepare data for modeling\n",
    "# We'll use all columns except those we know aren't features\n",
    "non_feature_cols = ['Country Name', 'Country Code', 'Year']\n",
    "feature_columns = [col for col in train_data.columns if col not in non_feature_cols]\n",
    "\n",
    "# Extract features from our already split datasets\n",
    "X_train = train_data[feature_columns].values\n",
    "X_val = val_data[feature_columns].values\n",
    "X_test = test_data[feature_columns].values\n",
    "\n",
    "# Extract the target variable - we focus on the selected target only\n",
    "y_train = train_targets[TARGET].values\n",
    "y_val = val_targets[TARGET].values\n",
    "y_test = test_targets[TARGET].values\n",
    "\n",
    "# Handle missing values in targets\n",
    "# For modeling, we need to drop rows with missing target values\n",
    "train_mask = ~np.isnan(y_train)\n",
    "val_mask = ~np.isnan(y_val)\n",
    "test_mask = ~np.isnan(y_test)\n",
    "\n",
    "# Apply the masks to both features and targets\n",
    "X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "X_val, y_val = X_val[val_mask], y_val[val_mask]\n",
    "X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "# Create a variable for the total dataset size (used in the summary)\n",
    "X_target = np.concatenate([X_train, X_val, X_test], axis=0)\n",
    "\n",
    "# Print information about the dataset\n",
    "print(f\"\\nData dimensions after removing missing targets:\")\n",
    "print(f\"Train Shape: {X_train.shape[0]:,} x {X_train.shape[1]:,}\")\n",
    "print(f\"Validation Shape: {X_val.shape[0]:,} x {X_val.shape[1]:,}\")\n",
    "print(f\"Test Shape: {X_test.shape[0]:,} x {X_test.shape[1]:,}\")\n",
    "print(f\"Train target elements: {y_train.size:,}\")\n",
    "print(f\"Validation target elements: {y_val.size:,}\")\n",
    "print(f\"Test target elements: {y_test.size:,}\")\n",
    "print(f\"Total observations: {X_target.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad58538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of features: 438\n",
      "Target: bottom10 (SI.DST.FRST.10)\n",
      "\n",
      "Countries with no bottom10 data: 76\n",
      "Sample of countries with no bottom10 data: ['MDG', 'MLT', 'PRY', 'DNK', 'OMN']\n"
     ]
    }
   ],
   "source": [
    "# Number of features to track\n",
    "print(f\"\\nNumber of features: {len(feature_columns)}\")\n",
    "print(f\"Target: {selected_target} ({TARGET})\")\n",
    "\n",
    "# Check for any countries that have no target values in any split\n",
    "train_countries_with_target = train_data.loc[train_mask, 'Country Code'].unique()\n",
    "val_countries_with_target = val_data.loc[val_mask, 'Country Code'].unique()\n",
    "test_countries_with_target = test_data.loc[test_mask, 'Country Code'].unique()\n",
    "\n",
    "all_countries = set(data['Country Code'].unique())\n",
    "countries_with_target = set(train_countries_with_target) | set(val_countries_with_target) | set(test_countries_with_target)\n",
    "countries_without_target = all_countries - countries_with_target\n",
    "\n",
    "print(f\"\\nCountries with no {selected_target} data: {len(countries_without_target)}\")\n",
    "if len(countries_without_target) > 0:\n",
    "    print(f\"Sample of countries with no {selected_target} data: {list(countries_without_target)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6021b2",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1513a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "# model = xgb.XGBRegressor(\n",
    "#     # Basic parameters\n",
    "#     objective='reg:squarederror',\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.05,  # Lower learning rate for better generalization\n",
    "    \n",
    "#     # Tree structure regularization\n",
    "#     max_depth=3,  # Lower max_depth to prevent complex trees\n",
    "#     min_child_weight=5,  # Higher values prevent overfitting on rare feature combinations\n",
    "#     gamma=1,  # Minimum loss reduction required for further partition\n",
    "    \n",
    "#     # Sampling regularization\n",
    "#     subsample=0.8,  # Train on 80% of data points each iteration\n",
    "#     colsample_bytree=0.7,  # Train each tree on 70% of features\n",
    "#     colsample_bylevel=0.7,  # Sample features at each level\n",
    "    \n",
    "#     # L1/L2 regularization\n",
    "#     reg_alpha=1.0,  # L1 regularization on weights\n",
    "#     reg_lambda=1.0,  # L2 regularization on weights\n",
    "    \n",
    "#     random_state=SEED\n",
    "# )\n",
    "\n",
    "# # Create a pipeline with scaling and the model\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('model', model),\n",
    "# ])\n",
    "\n",
    "# # Fit the model\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred_train = pipeline.predict(X_train)\n",
    "# y_pred_val = pipeline.predict(X_val)\n",
    "# y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "# train_r2 = r2_score(y_train, y_pred_train)\n",
    "# val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "# val_r2 = r2_score(y_val, y_pred_val)\n",
    "# test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "# test_r2 = r2_score(y_test, y_pred_test)\n",
    "# print(f\"\\nTrain MSE: {train_mse:.4f}, Train R2: {train_r2:.4f}\")\n",
    "# print(f\"Validation MSE: {val_mse:.4f}, Validation R2: {val_r2:.4f}\")\n",
    "# print(f\"Test MSE: {test_mse:.4f}, Test R2: {test_r2:.4f}\")\n",
    "\n",
    "# # Save the model\n",
    "# joblib.dump(pipeline, MODEL_FILE)\n",
    "# print(f\"\\nModel saved to {MODEL_FILE}\")\n",
    "\n",
    "# # Save the predictions\n",
    "# predictions_df = pd.DataFrame({\n",
    "#     'Actual': y_test,\n",
    "#     'Predicted': y_pred_test\n",
    "# })\n",
    "# predictions_df.to_csv(PREDICTIONS_FILE, index=False)\n",
    "# print(f\"\\nPredictions saved to {PREDICTIONS_FILE}\")\n",
    "\n",
    "# # Save the feature importances\n",
    "# importances = pipeline.named_steps['model'].feature_importances_\n",
    "# feature_importances_df = pd.DataFrame({\n",
    "#     'Feature': feature_columns,\n",
    "#     'Importance': importances\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "# feature_importances_df.to_csv(FEATURE_IMPORTANCES_FILE, index=False)\n",
    "# print(f\"\\nFeature importances saved to {FEATURE_IMPORTANCES_FILE}\")\n",
    "\n",
    "# print(\"\\nTop 10 feature importances:\")\n",
    "# feature_lookup = pd.read_csv(INDICATOR_LOOKUP_FILE)\n",
    "# feature_importances_df = feature_importances_df.merge(feature_lookup[['Indicator Code','Indicator Name']], how='left', left_on='Feature', right_on='Indicator Code')\n",
    "# display(feature_importances_df[['Indicator Code', 'Indicator Name', 'Importance']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3bf6d5",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a035450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "\n",
      "Best parameters: {'model__colsample_bylevel': 0.9760773204143818, 'model__colsample_bytree': 0.7892277714194997, 'model__gamma': 0.14046843914487694, 'model__learning_rate': 0.07814725395454945, 'model__max_depth': 3, 'model__min_child_weight': 3, 'model__n_estimators': 142, 'model__reg_alpha': 2.2350968296772424, 'model__reg_lambda': 0.6953962195156492, 'model__subsample': 0.774978624684772}\n",
      "Best cross-validation RMSE: 0.5634\n",
      "\n",
      "Train RMSE (best model): 0.2607, Train R2 (best model): 0.9303\n",
      "Validation RMSE (best model): 0.5406, Validation R2 (best model): 0.6749\n",
      "Test RMSE (best model): 0.5206, Test R2 (best model): 0.7187\n",
      "\n",
      "Best model saved to output/models/xgb_tuned_model_bottom10_2000_to_2020.pkl\n",
      "\n",
      "Best model predictions saved to output/predictions/xgb_tuned_predictions_bottom10_2000_to_2020.csv\n",
      "\n",
      "Best model feature importances saved to output/feature_importance/xgb_tuned_feature_importances_bottom10_2000_to_2020.csv\n",
      "\n",
      "Top 10 feature importances (best model):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Indicator Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Indicator Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Importance",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "31ea45a6-9c5f-4edc-b8e1-7b73ac257744",
       "rows": [
        [
         "0",
         "TX.VAL.AGRI.ZS.UN",
         "Agricultural raw materials exports (% of merchandise exports)",
         "0.029814895"
        ],
        [
         "1",
         "SH.DYN.AIDS.FE.ZS",
         "Women's share of population ages 15+ living with HIV (%)",
         "0.0272746"
        ],
        [
         "2",
         "ER.H2O.FWST.ZS",
         "Level of water stress: freshwater withdrawal as a proportion of available freshwater resources",
         "0.024809245"
        ],
        [
         "3",
         "IC.ISV.DURS",
         "Time to resolve insolvency (years)",
         "0.018700738"
        ],
        [
         "4",
         "GC.TAX.OTHR.CN",
         "Other taxes (current LCU)",
         "0.018099962"
        ],
        [
         "5",
         "NY.ADJ.DFOR.CD",
         "Adjusted savings: net forest depletion (current US$)",
         "0.016024731"
        ],
        [
         "6",
         "SH.MLR.INCD.P3",
         "Incidence of malaria (per 1,000 population at risk)",
         "0.013646855"
        ],
        [
         "7",
         "EG.ELC.RNWX.ZS",
         "Electricity production from renewable sources, excluding hydroelectric (% of total)",
         "0.013248136"
        ],
        [
         "8",
         "TX.VAL.MRCH.RS.ZS",
         "Merchandise exports by the reporting economy, residual (% of total merchandise exports)",
         "0.01307554"
        ],
        [
         "9",
         "DC.DAC.HUNL.CD",
         "Net bilateral aid flows from DAC donors, Hungary (current US$)",
         "0.011957173"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TX.VAL.AGRI.ZS.UN</td>\n",
       "      <td>Agricultural raw materials exports (% of merch...</td>\n",
       "      <td>0.029815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SH.DYN.AIDS.FE.ZS</td>\n",
       "      <td>Women's share of population ages 15+ living wi...</td>\n",
       "      <td>0.027275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ER.H2O.FWST.ZS</td>\n",
       "      <td>Level of water stress: freshwater withdrawal a...</td>\n",
       "      <td>0.024809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IC.ISV.DURS</td>\n",
       "      <td>Time to resolve insolvency (years)</td>\n",
       "      <td>0.018701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GC.TAX.OTHR.CN</td>\n",
       "      <td>Other taxes (current LCU)</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NY.ADJ.DFOR.CD</td>\n",
       "      <td>Adjusted savings: net forest depletion (curren...</td>\n",
       "      <td>0.016025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SH.MLR.INCD.P3</td>\n",
       "      <td>Incidence of malaria (per 1,000 population at ...</td>\n",
       "      <td>0.013647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EG.ELC.RNWX.ZS</td>\n",
       "      <td>Electricity production from renewable sources,...</td>\n",
       "      <td>0.013248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TX.VAL.MRCH.RS.ZS</td>\n",
       "      <td>Merchandise exports by the reporting economy, ...</td>\n",
       "      <td>0.013076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DC.DAC.HUNL.CD</td>\n",
       "      <td>Net bilateral aid flows from DAC donors, Hunga...</td>\n",
       "      <td>0.011957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Indicator Code                                     Indicator Name  \\\n",
       "0  TX.VAL.AGRI.ZS.UN  Agricultural raw materials exports (% of merch...   \n",
       "1  SH.DYN.AIDS.FE.ZS  Women's share of population ages 15+ living wi...   \n",
       "2     ER.H2O.FWST.ZS  Level of water stress: freshwater withdrawal a...   \n",
       "3        IC.ISV.DURS                 Time to resolve insolvency (years)   \n",
       "4     GC.TAX.OTHR.CN                          Other taxes (current LCU)   \n",
       "5     NY.ADJ.DFOR.CD  Adjusted savings: net forest depletion (curren...   \n",
       "6     SH.MLR.INCD.P3  Incidence of malaria (per 1,000 population at ...   \n",
       "7     EG.ELC.RNWX.ZS  Electricity production from renewable sources,...   \n",
       "8  TX.VAL.MRCH.RS.ZS  Merchandise exports by the reporting economy, ...   \n",
       "9     DC.DAC.HUNL.CD  Net bilateral aid flows from DAC donors, Hunga...   \n",
       "\n",
       "   Importance  \n",
       "0    0.029815  \n",
       "1    0.027275  \n",
       "2    0.024809  \n",
       "3    0.018701  \n",
       "4    0.018100  \n",
       "5    0.016025  \n",
       "6    0.013647  \n",
       "7    0.013248  \n",
       "8    0.013076  \n",
       "9    0.011957  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model parameters saved to output/models/xgb_tuned_model_params_bottom10_2000_to_2020.json\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'model__n_estimators': randint(50, 200),\n",
    "    'model__learning_rate': uniform(0.01, 0.1),  \n",
    "    'model__max_depth': randint(2, 6),           \n",
    "    'model__min_child_weight': randint(1, 6),    \n",
    "    'model__gamma': uniform(0, 2),               \n",
    "    'model__subsample': uniform(0.7, 0.3),       \n",
    "    'model__colsample_bytree': uniform(0.7, 0.3),\n",
    "    'model__colsample_bylevel': uniform(0.7, 0.3),\n",
    "    'model__reg_alpha': uniform(0, 3),           \n",
    "    'model__reg_lambda': uniform(0.5, 3)         \n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling and the model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED))\n",
    "])\n",
    "\n",
    "# Define the scoring function\n",
    "rmse_scorer = make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)), \n",
    "                          greater_is_better=False)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25,  # Number of parameter combinations to try\n",
    "    cv=5,       # 5-fold cross-validation\n",
    "    scoring=rmse_scorer,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "best_model = random_search.best_estimator_\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best cross-validation RMSE: {-best_score:.4f}\")\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_train_best = best_model.predict(X_train)\n",
    "y_pred_val_best = best_model.predict(X_val)\n",
    "y_pred_test_best = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "train_mse_best = mean_squared_error(y_train, y_pred_train_best)\n",
    "train_r2_best = r2_score(y_train, y_pred_train_best)\n",
    "val_mse_best = mean_squared_error(y_val, y_pred_val_best)\n",
    "val_r2_best = r2_score(y_val, y_pred_val_best)\n",
    "test_mse_best = mean_squared_error(y_test, y_pred_test_best)\n",
    "test_r2_best = r2_score(y_test, y_pred_test_best)\n",
    "\n",
    "# Add RMSE calculation\n",
    "train_rmse_best = np.sqrt(train_mse_best)\n",
    "val_rmse_best = np.sqrt(val_mse_best)\n",
    "test_rmse_best = np.sqrt(test_mse_best)\n",
    "\n",
    "print(f\"\\nTrain RMSE (best model): {train_rmse_best:.4f}, Train R2 (best model): {train_r2_best:.4f}\")\n",
    "print(f\"Validation RMSE (best model): {val_rmse_best:.4f}, Validation R2 (best model): {val_r2_best:.4f}\")\n",
    "print(f\"Test RMSE (best model): {test_rmse_best:.4f}, Test R2 (best model): {test_r2_best:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, BEST_MODEL_FILE)\n",
    "print(f\"\\nBest model saved to {BEST_MODEL_FILE}\")\n",
    "\n",
    "# Save the best model predictions\n",
    "predictions_best_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred_test_best\n",
    "})\n",
    "predictions_best_df.to_csv(BEST_PREDICTIONS_FILE, index=False)\n",
    "print(f\"\\nBest model predictions saved to {BEST_PREDICTIONS_FILE}\")\n",
    "\n",
    "# Save the best model feature importances\n",
    "importances_best = best_model.named_steps['model'].feature_importances_\n",
    "feature_importances_best_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': importances_best\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "feature_importances_best_df.to_csv(BEST_FEATURE_IMPORTANCES_FILE, index=False)\n",
    "print(f\"\\nBest model feature importances saved to {BEST_FEATURE_IMPORTANCES_FILE}\")\n",
    "\n",
    "print(\"\\nTop 10 feature importances (best model):\")\n",
    "feature_importances_best_df = feature_importances_best_df.merge(feature_lookup[['Indicator Code','Indicator Name']], how='left', left_on='Feature', right_on='Indicator Code')\n",
    "display(feature_importances_best_df[['Indicator Code', 'Indicator Name', 'Importance']].head(10))\n",
    "\n",
    "# Save the best model parameters\n",
    "with open(BEST_PARAMS_FILE, 'w') as f:\n",
    "    json.dump(best_params, f)\n",
    "print(f\"\\nBest model parameters saved to {BEST_PARAMS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22dea0",
   "metadata": {},
   "source": [
    "# SHAP Analysis (Raw Output Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630d646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAP values with country labels saved to output/feature_importance/shap_values_with_countries_bottom10_2000_to_2020.csv\n",
      "\n",
      "SHAP values saved to output/feature_importance/shap_values_bottom10_2000_to_2020.csv\n"
     ]
    }
   ],
   "source": [
    "# SHAP analysis - Raw Output Only\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the underlying XGBoost model from the pipeline\n",
    "xgb_model = best_model.named_steps['model']\n",
    "\n",
    "# Apply the scaler to transform the data first\n",
    "X_train_scaled = best_model.named_steps['scaler'].transform(X_train)\n",
    "X_test_scaled = best_model.named_steps['scaler'].transform(X_test)\n",
    "\n",
    "# Initialize SHAP explainer with the XGBoost model only\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Calculate SHAP values on the scaled data\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Get country information for the test set (after removing missing targets)\n",
    "# We need to get this from the original test_data DataFrame using the test_mask\n",
    "test_countries = test_data.loc[test_mask, ['Country Code', 'Country Name']].reset_index(drop=True)\n",
    "\n",
    "# Save SHAP values with country information\n",
    "shap_values_df = pd.DataFrame(shap_values, columns=feature_columns)\n",
    "shap_values_with_countries = pd.concat([test_countries, shap_values_df], axis=1)\n",
    "shap_values_with_countries.to_csv(f'{FEATURE_IMPORTANCE_DIR}/shap_values_with_countries_{FILE_SUFFIX}.csv', index=False)\n",
    "print(f\"\\nSHAP values with country labels saved to {FEATURE_IMPORTANCE_DIR}/shap_values_with_countries_{FILE_SUFFIX}.csv\")\n",
    "\n",
    "# Also save the original SHAP values without countries for backward compatibility\n",
    "shap_values_df.to_csv(f'{FEATURE_IMPORTANCE_DIR}/shap_values_{FILE_SUFFIX}.csv', index=False)\n",
    "print(f\"\\nSHAP values saved to {FEATURE_IMPORTANCE_DIR}/shap_values_{FILE_SUFFIX}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152e63a",
   "metadata": {},
   "source": [
    "# Export Data for Downstream Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5287a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country-feature SHAP matrix saved to output/feature_importance/ensemble_shap_values_bottom10_2000_to_2020.csv\n",
      "\n",
      "Exported SHAP matrix shape: (438, 87)\n",
      "Number of features: 438\n",
      "Number of countries: 87\n"
     ]
    }
   ],
   "source": [
    "# Export the aggregated SHAP values for use in downstream clustering\n",
    "# This creates a matrix of features x absolute SHAP values for all countries\n",
    "country_feature_impact = shap_values_with_countries.melt(\n",
    "    id_vars=['Country Code', 'Country Name'],\n",
    "    var_name='Feature',\n",
    "    value_name='SHAP Value'\n",
    ").pivot_table(\n",
    "    index='Feature',\n",
    "    columns='Country Code',\n",
    "    values='SHAP Value',\n",
    "    aggfunc='sum'  # Sum SHAP values per feature per country\n",
    ")\n",
    "\n",
    "# Save for downstream clustering\n",
    "country_feature_impact.to_csv(f'{FEATURE_IMPORTANCE_DIR}/ensemble_shap_values_{FILE_SUFFIX}.csv')\n",
    "print(f\"\\nCountry-feature SHAP matrix saved to {FEATURE_IMPORTANCE_DIR}/ensemble_shap_values_{FILE_SUFFIX}.csv\")\n",
    "\n",
    "# Show dimensions of the exported matrix\n",
    "print(f\"\\nExported SHAP matrix shape: {country_feature_impact.shape}\")\n",
    "print(f\"Number of features: {country_feature_impact.shape[0]}\")\n",
    "print(f\"Number of countries: {country_feature_impact.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f492c",
   "metadata": {},
   "source": [
    "# SHAP Values Final Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24af3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot shap values all three targets\n",
    "\n",
    "shap_top10 = pd.read_csv(f'{FEATURE_IMPORTANCE_DIR}/shap_values_top10_2000_to_2020.csv')\n",
    "shap_bottom10 = pd.read_csv(f'{FEATURE_IMPORTANCE_DIR}/shap_values_bottom10_2000_to_2020.csv')\n",
    "shap_gini = pd.read_csv(f'{FEATURE_IMPORTANCE_DIR}/shap_values_gini_2000_to_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3a24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_code_with_name(df, feature_lookup):\n",
    "    # Create a dictionary mapping from indicator codes to names\n",
    "    code_to_name = dict(zip(feature_lookup['Indicator Code'], feature_lookup['Indicator Name']))\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    renamed_df = df.copy()\n",
    "    \n",
    "    # For each column that exists in our lookup, rename it\n",
    "    columns_to_rename = {}\n",
    "    for col in renamed_df.columns:\n",
    "        if col in code_to_name:\n",
    "            columns_to_rename[col] = code_to_name[col]\n",
    "        # Keep non-indicator columns unchanged\n",
    "        # (like 'Country Code', 'Country Name')\n",
    "    \n",
    "    # Apply the renaming\n",
    "    renamed_df = renamed_df.rename(columns=columns_to_rename)\n",
    "    \n",
    "    return renamed_df\n",
    "\n",
    "# Replace codes with names in the SHAP DataFrames\n",
    "shap_top10 = replace_code_with_name(shap_top10, feature_lookup)\n",
    "shap_bottom10 = replace_code_with_name(shap_bottom10, feature_lookup)\n",
    "shap_gini = replace_code_with_name(shap_gini, feature_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa9cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features for GINI coefficient:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Women's share of population ages 15+ living with HIV (%)                               0.891842\n",
       "Employment in industry, female (% of female employment) (modeled ILO estimate)         0.768996\n",
       "Prevalence of HIV, total (% of population ages 15-49)                                  0.652474\n",
       "Manufacturing, value added (% of GDP)                                                  0.583211\n",
       "Agricultural land (% of land area)                                                     0.554780\n",
       "Time to resolve insolvency (years)                                                     0.385566\n",
       "Procedures to register property (number)                                               0.376114\n",
       "Agricultural raw materials exports (% of merchandise exports)                          0.375645\n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - Deforestation (Mt CO2e)                  0.276017\n",
       "Merchandise exports to economies in the Arab World (% of total merchandise exports)    0.265329\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features for Top 10% wealth share:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Women's share of population ages 15+ living with HIV (%)                                      0.987732\n",
       "Employment in industry, female (% of female employment) (modeled ILO estimate)                0.602129\n",
       "Manufacturing, value added (% of GDP)                                                         0.455765\n",
       "Prevalence of HIV, total (% of population ages 15-49)                                         0.423031\n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - Deforestation (Mt CO2e)                         0.386342\n",
       "Employers, female (% of female employment) (modeled ILO estimate)                             0.302864\n",
       "Agricultural land (% of land area)                                                            0.230310\n",
       "Net lending (+) / net borrowing (-) (% of GDP)                                                0.207019\n",
       "Merchandise exports to economies in the Arab World (% of total merchandise exports)           0.176504\n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - Total excluding non-tropical fires (Mt CO2e)    0.164341\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features for Bottom 10% wealth share:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Agricultural raw materials exports (% of merchandise exports)                            0.128460\n",
       "Women's share of population ages 15+ living with HIV (%)                                 0.072848\n",
       "Population in urban agglomerations of more than 1 million                                0.071998\n",
       "Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, male (%)       0.060867\n",
       "Incidence of malaria (per 1,000 population at risk)                                      0.052865\n",
       "Manufacturing, value added (% of GDP)                                                    0.045395\n",
       "Employment in industry, female (% of female employment) (modeled ILO estimate)           0.045185\n",
       "Statistical performance indicators (SPI): Pillar 3 data products score  (scale 0-100)    0.044650\n",
       "Time to resolve insolvency (years)                                                       0.041506\n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - Deforestation (Mt CO2e)                    0.041300\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of top feature importance values:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GINI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Top 10%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Bottom 10%",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d59f6c6b-9c01-4968-bc63-0750e25789ad",
       "rows": [
        [
         "Agricultural land (% of land area)",
         "0.5547796215626667",
         "0.23031007048133337",
         null
        ],
        [
         "Agricultural raw materials exports (% of merchandise exports)",
         "0.37564474425333333",
         null,
         "0.12846014871333333"
        ],
        [
         "Carbon dioxide (CO2) net fluxes from LULUCF - Deforestation (Mt CO2e)",
         "0.27601723770200004",
         "0.3863423330933333",
         "0.04130035345333334"
        ],
        [
         "Carbon dioxide (CO2) net fluxes from LULUCF - Total excluding non-tropical fires (Mt CO2e)",
         null,
         "0.164340542788",
         null
        ],
        [
         "Employers, female (% of female employment) (modeled ILO estimate)",
         null,
         "0.30286355136",
         null
        ],
        [
         "Employment in industry, female (% of female employment) (modeled ILO estimate)",
         "0.7689956794666667",
         "0.60212866147",
         "0.0451848521634"
        ],
        [
         "Incidence of malaria (per 1,000 population at risk)",
         null,
         null,
         "0.05286473219666667"
        ],
        [
         "Manufacturing, value added (% of GDP)",
         "0.5832105904",
         "0.4557647095933334",
         "0.04539487410666667"
        ],
        [
         "Merchandise exports to economies in the Arab World (% of total merchandise exports)",
         "0.2653286076954",
         "0.17650418363066664",
         null
        ],
        [
         "Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, male (%)",
         null,
         null,
         "0.06086701484666667"
        ],
        [
         "Net lending (+) / net borrowing (-) (% of GDP)",
         null,
         "0.20701904895399997",
         null
        ],
        [
         "Population in urban agglomerations of more than 1 million",
         null,
         null,
         "0.07199845277333333"
        ],
        [
         "Prevalence of HIV, total (% of population ages 15-49)",
         "0.6524742372666666",
         "0.42303108163600006",
         null
        ],
        [
         "Procedures to register property (number)",
         "0.3761141965333333",
         null,
         null
        ],
        [
         "Statistical performance indicators (SPI): Pillar 3 data products score  (scale 0-100)",
         null,
         null,
         "0.044650079432"
        ],
        [
         "Time to resolve insolvency (years)",
         "0.38556594249399995",
         null,
         "0.04150562631133333"
        ],
        [
         "Women's share of population ages 15+ living with HIV (%)",
         "0.8918424481333334",
         "0.9877316191320001",
         "0.07284813748666669"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 17
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GINI</th>\n",
       "      <th>Top 10%</th>\n",
       "      <th>Bottom 10%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agricultural land (% of land area)</th>\n",
       "      <td>0.554780</td>\n",
       "      <td>0.230310</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural raw materials exports (% of merchandise exports)</th>\n",
       "      <td>0.375645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.128460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon dioxide (CO2) net fluxes from LULUCF - Deforestation (Mt CO2e)</th>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.386342</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon dioxide (CO2) net fluxes from LULUCF - Total excluding non-tropical fires (Mt CO2e)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.164341</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employers, female (% of female employment) (modeled ILO estimate)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.302864</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employment in industry, female (% of female employment) (modeled ILO estimate)</th>\n",
       "      <td>0.768996</td>\n",
       "      <td>0.602129</td>\n",
       "      <td>0.045185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incidence of malaria (per 1,000 population at risk)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manufacturing, value added (% of GDP)</th>\n",
       "      <td>0.583211</td>\n",
       "      <td>0.455765</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Merchandise exports to economies in the Arab World (% of total merchandise exports)</th>\n",
       "      <td>0.265329</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, male (%)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Net lending (+) / net borrowing (-) (% of GDP)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.207019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population in urban agglomerations of more than 1 million</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence of HIV, total (% of population ages 15-49)</th>\n",
       "      <td>0.652474</td>\n",
       "      <td>0.423031</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Procedures to register property (number)</th>\n",
       "      <td>0.376114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical performance indicators (SPI): Pillar 3 data products score  (scale 0-100)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time to resolve insolvency (years)</th>\n",
       "      <td>0.385566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Women's share of population ages 15+ living with HIV (%)</th>\n",
       "      <td>0.891842</td>\n",
       "      <td>0.987732</td>\n",
       "      <td>0.072848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        GINI   Top 10%  \\\n",
       "Agricultural land (% of land area)                  0.554780  0.230310   \n",
       "Agricultural raw materials exports (% of mercha...  0.375645       NaN   \n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - D...  0.276017  0.386342   \n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - T...       NaN  0.164341   \n",
       "Employers, female (% of female employment) (mod...       NaN  0.302864   \n",
       "Employment in industry, female (% of female emp...  0.768996  0.602129   \n",
       "Incidence of malaria (per 1,000 population at r...       NaN       NaN   \n",
       "Manufacturing, value added (% of GDP)               0.583211  0.455765   \n",
       "Merchandise exports to economies in the Arab Wo...  0.265329  0.176504   \n",
       "Mortality from CVD, cancer, diabetes or CRD bet...       NaN       NaN   \n",
       "Net lending (+) / net borrowing (-) (% of GDP)           NaN  0.207019   \n",
       "Population in urban agglomerations of more than...       NaN       NaN   \n",
       "Prevalence of HIV, total (% of population ages ...  0.652474  0.423031   \n",
       "Procedures to register property (number)            0.376114       NaN   \n",
       "Statistical performance indicators (SPI): Pilla...       NaN       NaN   \n",
       "Time to resolve insolvency (years)                  0.385566       NaN   \n",
       "Women's share of population ages 15+ living wit...  0.891842  0.987732   \n",
       "\n",
       "                                                    Bottom 10%  \n",
       "Agricultural land (% of land area)                         NaN  \n",
       "Agricultural raw materials exports (% of mercha...    0.128460  \n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - D...    0.041300  \n",
       "Carbon dioxide (CO2) net fluxes from LULUCF - T...         NaN  \n",
       "Employers, female (% of female employment) (mod...         NaN  \n",
       "Employment in industry, female (% of female emp...    0.045185  \n",
       "Incidence of malaria (per 1,000 population at r...    0.052865  \n",
       "Manufacturing, value added (% of GDP)                 0.045395  \n",
       "Merchandise exports to economies in the Arab Wo...         NaN  \n",
       "Mortality from CVD, cancer, diabetes or CRD bet...    0.060867  \n",
       "Net lending (+) / net borrowing (-) (% of GDP)             NaN  \n",
       "Population in urban agglomerations of more than...    0.071998  \n",
       "Prevalence of HIV, total (% of population ages ...         NaN  \n",
       "Procedures to register property (number)                   NaN  \n",
       "Statistical performance indicators (SPI): Pilla...    0.044650  \n",
       "Time to resolve insolvency (years)                    0.041506  \n",
       "Women's share of population ages 15+ living wit...    0.072848  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Function to get the top 10 features by SHAP value magnitude\n",
    "def get_top_features(shap_df, n=10):\n",
    "    # Calculate the mean absolute SHAP value for each feature across all countries\n",
    "    feature_importance = pd.Series(np.abs(shap_df).mean(), index=shap_df.columns)\n",
    "    \n",
    "    # Sort by absolute importance and get top n\n",
    "    top_features = feature_importance.sort_values(ascending=False).head(n)\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "# Get top 10 features for each target variable\n",
    "top_gini_features = get_top_features(shap_gini)\n",
    "top_top10_features = get_top_features(shap_top10)\n",
    "top_bottom10_features = get_top_features(shap_bottom10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 features for GINI coefficient:\")\n",
    "display(top_gini_features)\n",
    "\n",
    "print(\"\\nTop 10 features for Top 10% wealth share:\")\n",
    "display(top_top10_features)\n",
    "\n",
    "print(\"\\nTop 10 features for Bottom 10% wealth share:\")\n",
    "display(top_bottom10_features)\n",
    "\n",
    "# Create a comparison DataFrame to see overlap between important features\n",
    "comparison = pd.DataFrame({\n",
    "    'GINI': top_gini_features,\n",
    "    'Top 10%': top_top10_features,\n",
    "    'Bottom 10%': top_bottom10_features\n",
    "})\n",
    "\n",
    "print(\"\\nComparison of top feature importance values:\")\n",
    "display(comparison)\n",
    "comparison.to_csv(f'{FEATURE_IMPORTANCE_DIR}/top_features_comparison_xgboost_rs_2000_to_2020.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
