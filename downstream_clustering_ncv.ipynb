{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7b3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618a5bf",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac37b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "DATE_RANGE = '2000_to_2020'\n",
    "\n",
    "DATA_PATH = f'input/imputed/df_wide_knn_imputed_{DATE_RANGE}.csv'\n",
    "FEATURE_IMPORTANCE_PATH = f'output/feature_importance_NCV/ensemble_feature_importances_bottom10_{DATE_RANGE}.csv'\n",
    "\n",
    "# PCA params\n",
    "N_FEATURES = 10\n",
    "N_COMPONENTS = 2\n",
    "\n",
    "# DBSCAN params\n",
    "EPSILON = 0.2\n",
    "MIN_SAMPLES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d3067",
   "metadata": {},
   "source": [
    "# Pull Data and Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12483c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SL.IND.EMPL.FE.ZS', 'SH.DYN.AIDS.FE.ZS', 'SH.DYN.NCOM.MA.ZS', 'TX.VAL.AGRI.ZS.UN', 'IC.ISV.DURS', 'SL.UEM.NEET.FE.ZS', 'SE.SEC.PROG.MA.ZS', 'IC.REG.COST.PC.ZS', 'SE.SEC.PRIV.ZS', 'AG.LND.ARBL.HA_lag1']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['AG.LND.ARBL.HA_lag1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m top_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FEATURE_IMPORTANCE_PATH)\u001b[38;5;241m.\u001b[39mhead(N_FEATURES)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(top_features)\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCountry Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCountry Code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display(data)\n\u001b[1;32m      7\u001b[0m data_features \u001b[38;5;241m=\u001b[39m data[top_features]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[1;32m    139\u001b[0m ):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:979\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    977\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    982\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['AG.LND.ARBL.HA_lag1']"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "top_features = pd.read_csv(FEATURE_IMPORTANCE_PATH).head(N_FEATURES)['Feature'].tolist()\n",
    "print(top_features)\n",
    "\n",
    "data = pd.read_csv(DATA_PATH, usecols=['Country Name', 'Country Code', 'Year'] + top_features)\n",
    "display(data)\n",
    "data_features = data[top_features]\n",
    "data_labels = data[['Country Name', 'Country Code', 'Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs if applicable\n",
    "print(f\"Shape of data: {data_features.shape}\")\n",
    "print(f\"NAs: {data_features.isna().sum().sum():,}\")\n",
    "\n",
    "# drop NAs\n",
    "data_features = data_features.dropna()\n",
    "\n",
    "print(f\"\\nShape of data after dropping NAs: {data_features.shape}\")\n",
    "print(f\"NAs: {data_features.isna().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d3bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_features)\n",
    "print(f\"\\nShape of scaled data: {data_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff24eaf",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with dynamic number of components\n",
    "pca = PCA(n_components=N_COMPONENTS)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Create column names for principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(N_COMPONENTS)]\n",
    "data_pca_df = pd.DataFrame(data_pca, columns=pca_columns)\n",
    "\n",
    "# Combine with labels\n",
    "data_pca_df = pd.concat([data_labels, data_pca_df], axis=1)\n",
    "display(data_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs('output/pca', exist_ok=True)\n",
    "\n",
    "# Calculate PCA evaluation metrics\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "# Create a dataframe to display the metrics\n",
    "pc_df = pd.DataFrame({\n",
    "    'Principal Component': [f'PC{i+1}' for i in range(N_COMPONENTS)],\n",
    "    'Explained Variance Ratio (%)': explained_variance * 100,\n",
    "    'Cumulative Explained Variance (%)': cumulative_variance * 100\n",
    "})\n",
    "\n",
    "# Display the metrics as a table\n",
    "display(pc_df)\n",
    "\n",
    "# Create a bar chart to visualize explained variance\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot for individual explained variance\n",
    "ax[0].bar(pca_columns, explained_variance * 100)\n",
    "ax[0].set_ylabel('Explained Variance Ratio (%)')\n",
    "ax[0].set_title('Explained Variance by Principal Component')\n",
    "for i, v in enumerate(explained_variance):\n",
    "    ax[0].text(i, v*100 + 1, f'{v*100:.1f}%', ha='center')\n",
    "\n",
    "# Line plot for cumulative explained variance\n",
    "ax[1].plot(pca_columns, cumulative_variance * 100, 'o-', color='green')\n",
    "ax[1].set_ylabel('Cumulative Explained Variance (%)')\n",
    "ax[1].set_title('Cumulative Explained Variance')\n",
    "for i, v in enumerate(cumulative_variance):\n",
    "    ax[1].text(i, v*100 + 1, f'{v*100:.1f}%', ha='center')\n",
    "ax[1].axhline(y=80, color='r', linestyle='--', alpha=0.5, label='80% Threshold')\n",
    "ax[1].axhline(y=90, color='b', linestyle='--', alpha=0.5, label='90% Threshold')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'output/pca/pca{N_COMPONENTS}_explained_variance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bd15f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "dbscan = DBSCAN(eps=EPSILON, min_samples=MIN_SAMPLES)\n",
    "\n",
    "for year in data_labels['Year'].unique():\n",
    "    dbscan.fit(data_pca_df[data_pca_df['Year'] == year][pca_columns])\n",
    "    data_pca_df.loc[data_pca_df['Year'] == year, 'Cluster'] = dbscan.labels_\n",
    "    silhouette_avg = silhouette_score(data_pca_df[data_pca_df['Year'] == year][pca_columns], dbscan.labels_)\n",
    "    print(f\"Year: {year}, Silhouette Score: {silhouette_avg:.2f}\")\n",
    "\n",
    "# Save the clustered data\n",
    "data_pca_df.to_csv(f'output/pca/clustered_data_{DATE_RANGE}.csv', index=False)\n",
    "display(data_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f7773",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6519b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_cluster_plot(data_pca_df, hide_outliers=True):\n",
    "    # Get unique years in sorted order\n",
    "    years = sorted(data_pca_df['Year'].unique())\n",
    "    print(f\"Years in dataset: {years}\")\n",
    "    \n",
    "    # Create a copy of the dataframe to modify for plotting\n",
    "    plot_df = data_pca_df.copy()\n",
    "    \n",
    "    # Make sure the Cluster column is numeric\n",
    "    if 'Cluster' not in plot_df.columns:\n",
    "        print(\"Error: Cluster column not found!\")\n",
    "        return None\n",
    "        \n",
    "    # Check for any NaN values in Cluster column\n",
    "    if plot_df['Cluster'].isna().any():\n",
    "        print(f\"Warning: Found {plot_df['Cluster'].isna().sum()} NaN values in Cluster column\")\n",
    "        plot_df['Cluster'] = plot_df['Cluster'].fillna(-1)  # Fill NaNs with -1 (outliers)\n",
    "    \n",
    "    # Ensure Cluster is integer type\n",
    "    plot_df['Cluster'] = plot_df['Cluster'].astype(int)\n",
    "    \n",
    "    # Print cluster statistics per year to help debug\n",
    "    for year in years:\n",
    "        year_data = plot_df[plot_df['Year'] == year]\n",
    "        clusters = sorted(year_data['Cluster'].unique())\n",
    "        print(f\"Year {year}: {len(year_data)} countries, Clusters: {clusters}\")\n",
    "    \n",
    "    # Remove outliers if requested\n",
    "    if hide_outliers:\n",
    "        plot_df = plot_df[plot_df['Cluster'] != -1]\n",
    "        print(f\"Outliers hidden. New shape: {plot_df.shape}\")\n",
    "    \n",
    "    # Add a column for custom coloring - convert cluster -1 (outliers) to 'outlier' if they exist\n",
    "    plot_df['Cluster_Label'] = plot_df['Cluster'].apply(lambda x: 'outlier' if x == -1 else f'cluster_{int(x)}')\n",
    "    \n",
    "    # Get all unique cluster labels for consistent color mapping\n",
    "    all_clusters = set()\n",
    "    for year in years:\n",
    "        year_data = plot_df[plot_df['Year'] == year]\n",
    "        clusters = year_data['Cluster'].unique()\n",
    "        all_clusters.update([f'cluster_{int(c)}' for c in clusters if c != -1])\n",
    "    all_clusters = sorted(list(all_clusters))\n",
    "    # Only add outlier category if we're showing outliers\n",
    "    if not hide_outliers:\n",
    "        all_clusters.append('outlier')  # Add outlier at the end\n",
    "    \n",
    "    # Create figure with custom color mapping\n",
    "    fig = px.scatter(plot_df, x='PC1', y='PC2', color='Cluster_Label',\n",
    "                     hover_name='Country Name', animation_frame='Year',\n",
    "                     color_discrete_map={'outlier': 'grey'},\n",
    "                     category_orders={'Cluster_Label': all_clusters})\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"PCA Clusters Over Time {'(Outliers Hidden)' if hide_outliers else ''}\",\n",
    "        xaxis_title='Principal Component 1',\n",
    "        yaxis_title='Principal Component 2',\n",
    "        legend_title='Cluster',\n",
    "        height=700,\n",
    "        width=1000,\n",
    "        dragmode='pan',\n",
    "        hovermode='closest',\n",
    "        updatemenus=[\n",
    "            # Play/pause animation buttons\n",
    "            {\n",
    "                'type': 'buttons',\n",
    "                'showactive': False,\n",
    "                'y': 0,\n",
    "                'x': 1.05,\n",
    "                'xanchor': 'left',\n",
    "                'yanchor': 'bottom',\n",
    "                'buttons': [{\n",
    "                    'method': 'animate',\n",
    "                    'label': 'Play',\n",
    "                    'args': [None, {\n",
    "                        'frame': {'duration': 1000, 'redraw': True},\n",
    "                        'fromcurrent': True,\n",
    "                    }]\n",
    "                }, {\n",
    "                    'method': 'animate',\n",
    "                    'label': 'Pause',\n",
    "                    'args': [[None], {\n",
    "                        'frame': {'duration': 0, 'redraw': False},\n",
    "                        'mode': 'immediate',\n",
    "                        'transition': {'duration': 0}\n",
    "                    }]\n",
    "                }]\n",
    "            }\n",
    "        ],\n",
    "        sliders=[{\n",
    "            'active': 0,\n",
    "            'yanchor': 'top',\n",
    "            'xanchor': 'left',\n",
    "            'currentvalue': {\n",
    "                'font': {'size': 16},\n",
    "                'prefix': 'Year: ',\n",
    "                'visible': True,\n",
    "                'xanchor': 'right'\n",
    "            },\n",
    "            'transition': {'duration': 500},\n",
    "            'pad': {'b': 10, 't': 50},\n",
    "            'len': 0.9,\n",
    "            'x': 0.1,\n",
    "            'y': 0,\n",
    "            'steps': [{\n",
    "                'args': [[str(year)], {  # Convert to string to match frame keys\n",
    "                    'frame': {'duration': 500, 'redraw': True},\n",
    "                    'mode': 'immediate',\n",
    "                    'transition': {'duration': 500}\n",
    "                }],\n",
    "                'label': str(year),\n",
    "                'method': 'animate'\n",
    "            } for year in years]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Update traces - make outlier points smaller and more transparent (if shown)\n",
    "    for frame in fig.frames:\n",
    "        for i, trace in enumerate(frame.data):\n",
    "            if hasattr(trace, 'name') and trace.name == 'outlier':\n",
    "                trace.marker = dict(size=6, opacity=0.5)\n",
    "            else:\n",
    "                trace.marker = dict(size=10, opacity=0.8)\n",
    "    \n",
    "    # Update main figure markers too\n",
    "    for i, trace in enumerate(fig.data):\n",
    "        if hasattr(trace, 'name') and trace.name == 'outlier':\n",
    "            fig.data[i].marker = dict(size=6, opacity=0.5)\n",
    "        else:\n",
    "            fig.data[i].marker = dict(size=10, opacity=0.8)\n",
    "    \n",
    "    # Improve appearance with white background and grid\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        xaxis=dict(showgrid=True, gridwidth=0.5, gridcolor='lightgray'),\n",
    "        yaxis=dict(showgrid=True, gridwidth=0.5, gridcolor='lightgray')\n",
    "    )\n",
    "    \n",
    "    # Add modebar buttons configuration\n",
    "    fig.update_layout(\n",
    "        modebar_add=['pan2d', 'zoom2d', 'resetScale2d', 'toImage'],\n",
    "        modebar_remove=['lasso2d', 'select2d'], # Remove lasso and box selection tools\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the interactive plot with outliers hidden\n",
    "interactive_fig = create_interactive_cluster_plot(data_pca_df, hide_outliers=True)\n",
    "interactive_fig.show()\n",
    "\n",
    "# Save the interactive plot as HTML for sharing\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "interactive_fig.write_html(f'output/plots/interactive_clusters_{DATE_RANGE}_no_outliers.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create another version with outliers visible for comparison\n",
    "interactive_fig_with_outliers = create_interactive_cluster_plot(data_pca_df, hide_outliers=False)\n",
    "interactive_fig_with_outliers.show()\n",
    "\n",
    "# Save this version too\n",
    "interactive_fig_with_outliers.write_html(f'output/plots/interactive_clusters_{DATE_RANGE}_with_outliers.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1935ee",
   "metadata": {},
   "source": [
    "# Cluster Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster_stability(data_pca_df):\n",
    "    years = sorted(data_pca_df['Year'].unique())\n",
    "    countries = data_pca_df['Country Name'].unique()\n",
    "    \n",
    "    # Create a DataFrame to track cluster changes\n",
    "    cluster_tracking = pd.DataFrame(index=countries)\n",
    "    \n",
    "    # Fill the DataFrame with cluster assignments for each year\n",
    "    for year in years:\n",
    "        year_data = data_pca_df[data_pca_df['Year'] == year]\n",
    "        year_clusters = dict(zip(year_data['Country Name'], year_data['Cluster']))\n",
    "        cluster_tracking[str(year)] = cluster_tracking.index.map(year_clusters)\n",
    "    \n",
    "    # Calculate the number of cluster changes for each country\n",
    "    cluster_tracking['changes'] = cluster_tracking.apply(\n",
    "        lambda row: sum(row[str(years[i])] != row[str(years[i+1])] \n",
    "                      for i in range(len(years)-1) \n",
    "                      if not pd.isna(row[str(years[i])]) and not pd.isna(row[str(years[i+1])])),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Sort by number of changes\n",
    "    cluster_tracking = cluster_tracking.sort_values('changes', ascending=False)\n",
    "    \n",
    "    # Display countries with the most and least cluster changes\n",
    "    print(\"Countries with the most cluster changes:\")\n",
    "    display(cluster_tracking.head(10))\n",
    "    \n",
    "    print(\"\\nCountries with the fewest cluster changes (excluding no data):\")\n",
    "    display(cluster_tracking[cluster_tracking['changes'] > 0].tail(10))\n",
    "    \n",
    "    # Calculate percentage of countries that changed clusters between consecutive years\n",
    "    change_percentages = []\n",
    "    for i in range(len(years)-1):\n",
    "        year1, year2 = str(years[i]), str(years[i+1])\n",
    "        valid_rows = cluster_tracking[[year1, year2]].dropna().shape[0]\n",
    "        changes = sum(cluster_tracking[year1] != cluster_tracking[year2])\n",
    "        change_percentages.append((years[i], years[i+1], changes / valid_rows * 100 if valid_rows > 0 else 0))\n",
    "    \n",
    "    # Plot the percentage of countries changing clusters over time\n",
    "    change_df = pd.DataFrame(change_percentages, columns=['Year1', 'Year2', 'Change Percentage'])\n",
    "    fig = px.line(change_df, x='Year1', y='Change Percentage', markers=True,\n",
    "                 title='Percentage of Countries Changing Clusters Between Consecutive Years')\n",
    "    fig.update_layout(xaxis_title='Year', yaxis_title='Percentage Changed (%)')\n",
    "    fig.show()\n",
    "    \n",
    "    return cluster_tracking\n",
    "\n",
    "# Run the stability analysis\n",
    "cluster_stability = analyze_cluster_stability(data_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedff26c",
   "metadata": {},
   "source": [
    "# Enhanced Cluster Community Stability Analysis\n",
    "\n",
    "While the previous analysis tracks the numerical changes in cluster assignments, it doesn't capture whether countries stay together as a \"community\" over time. Below, we analyze the stability of country groupings regardless of the cluster number assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a30bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster_community_stability(data_pca_df):\n",
    "    years = sorted(data_pca_df['Year'].unique())\n",
    "    countries = data_pca_df['Country Name'].unique()\n",
    "    \n",
    "    # Dictionary to store cluster compositions for each year\n",
    "    cluster_compositions = {}\n",
    "    \n",
    "    # Build cluster membership by year\n",
    "    for year in years:\n",
    "        year_data = data_pca_df[data_pca_df['Year'] == year]\n",
    "        clusters = {}\n",
    "        for cluster_id in sorted(year_data['Cluster'].unique()):\n",
    "            if cluster_id == -1:  # Skip outliers\n",
    "                continue\n",
    "            cluster_countries = set(year_data[year_data['Cluster'] == cluster_id]['Country Name'])\n",
    "            if len(cluster_countries) >= 3:  # Only consider clusters with at least 3 countries\n",
    "                clusters[int(cluster_id)] = cluster_countries\n",
    "        cluster_compositions[year] = clusters\n",
    "    \n",
    "    # Calculate Jaccard similarity between consecutive years' clusters\n",
    "    def jaccard_similarity(set1, set2):\n",
    "        if not set1 or not set2:\n",
    "            return 0\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union if union > 0 else 0\n",
    "    \n",
    "    # For each year pair, find the best matching clusters\n",
    "    community_stability = []\n",
    "    for i in range(len(years)-1):\n",
    "        year1, year2 = years[i], years[i+1]\n",
    "        clusters1 = cluster_compositions[year1]\n",
    "        clusters2 = cluster_compositions[year2]\n",
    "        \n",
    "        if not clusters1 or not clusters2:\n",
    "            continue\n",
    "            \n",
    "        # Track matched clusters to avoid double-matching\n",
    "        matched_clusters2 = set()\n",
    "        total_similarity = 0\n",
    "        match_count = 0\n",
    "        cluster_matches = []\n",
    "        \n",
    "        # For each cluster in year1, find best match in year2\n",
    "        for c1_id, c1_countries in clusters1.items():\n",
    "            best_similarity = 0\n",
    "            best_match = None\n",
    "            \n",
    "            for c2_id, c2_countries in clusters2.items():\n",
    "                if c2_id in matched_clusters2:\n",
    "                    continue\n",
    "                    \n",
    "                similarity = jaccard_similarity(c1_countries, c2_countries)\n",
    "                if similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_match = c2_id\n",
    "            \n",
    "            if best_match is not None and best_similarity >= 0.3:  # Threshold for considering a match\n",
    "                matched_clusters2.add(best_match)\n",
    "                total_similarity += best_similarity\n",
    "                match_count += 1\n",
    "                cluster_matches.append((c1_id, best_match, best_similarity, len(clusters1[c1_id])))\n",
    "        \n",
    "        # Calculate average stability between these two years\n",
    "        avg_stability = total_similarity / len(clusters1) if clusters1 else 0\n",
    "        community_stability.append((year1, year2, avg_stability, match_count, len(clusters1), len(clusters2)))\n",
    "        \n",
    "        # Print detailed matching information for significant changes\n",
    "        if avg_stability < 0.5:  # Highlight years with low stability\n",
    "            print(f\"\\nLow stability between {year1} and {year2} (avg: {avg_stability:.2f})\")\n",
    "            for c1_id, c2_id, sim, size in sorted(cluster_matches, key=lambda x: x[2]):\n",
    "                print(f\"  Cluster {c1_id} â†’ {c2_id}: similarity {sim:.2f} (size: {size})\")\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    stability_df = pd.DataFrame(community_stability, columns=[\n",
    "        'Year1', 'Year2', 'Community_Stability', 'Matches', 'Clusters_Year1', 'Clusters_Year2'\n",
    "    ])\n",
    "    \n",
    "    # Plot community stability over time\n",
    "    fig = px.line(stability_df, x='Year1', y='Community_Stability', markers=True,\n",
    "                 title='Cluster Community Stability Between Consecutive Years')\n",
    "    fig.update_layout(xaxis_title='Year', yaxis_title='Average Community Stability (0-1)',\n",
    "                     yaxis=dict(range=[0, 1]))\n",
    "    fig.show()\n",
    "    \n",
    "    # Plot cluster count over time\n",
    "    cluster_counts = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Cluster Count': [len(cluster_compositions[year]) for year in years]\n",
    "    })\n",
    "    fig = px.bar(cluster_counts, x='Year', y='Cluster Count',\n",
    "                title='Number of Clusters per Year')\n",
    "    fig.update_layout(xaxis_title='Year', yaxis_title='Number of Clusters')\n",
    "    fig.show()\n",
    "    \n",
    "    # Create a consistency score for each country\n",
    "    country_consistency = {country: 0 for country in countries}\n",
    "    country_transitions = {country: [] for country in countries}\n",
    "    \n",
    "    # Track which community each country belongs to over time\n",
    "    country_communities = {country: {} for country in countries}\n",
    "    \n",
    "    # First, identify the community for each country in each year\n",
    "    for year in years:\n",
    "        year_data = data_pca_df[data_pca_df['Year'] == year]\n",
    "        for _, row in year_data.iterrows():\n",
    "            country = row['Country Name']\n",
    "            cluster = row['Cluster']\n",
    "            if cluster != -1:  # Ignore outliers\n",
    "                country_communities[country][year] = cluster\n",
    "                if country_transitions[country] and country_transitions[country][-1][1] != cluster:\n",
    "                    country_transitions[country].append((year, cluster))\n",
    "                elif not country_transitions[country]:\n",
    "                    country_transitions[country].append((year, cluster))\n",
    "    \n",
    "    # Calculate consistency: how often a country stays with the same set of neighbors\n",
    "    country_consistency_scores = []\n",
    "    for country in countries:\n",
    "        years_present = len(country_communities[country])\n",
    "        if years_present <= 1:\n",
    "            continue  # Skip countries with insufficient data\n",
    "            \n",
    "        # Look at each consecutive year pair\n",
    "        consistency_count = 0\n",
    "        total_comparisons = 0\n",
    "        \n",
    "        for i in range(len(years)-1):\n",
    "            year1, year2 = years[i], years[i+1]\n",
    "            if year1 not in country_communities[country] or year2 not in country_communities[country]:\n",
    "                continue\n",
    "                \n",
    "            # Get other countries in the same cluster in both years\n",
    "            cluster1 = country_communities[country][year1]\n",
    "            cluster2 = country_communities[country][year2]\n",
    "            countries1 = set(year_data[year_data['Cluster'] == cluster1]['Country Name'])\n",
    "            countries2 = set(year_data[year_data['Cluster'] == cluster2]['Country Name'])\n",
    "            \n",
    "            # Calculate consistency: what fraction of neighbors stayed together\n",
    "            common_neighbors = len(countries1.intersection(countries2)) - 1  # exclude self\n",
    "            all_neighbors = len(countries1.union(countries2)) - 1  # exclude self\n",
    "            if all_neighbors > 0:\n",
    "                consistency = common_neighbors / all_neighbors\n",
    "                consistency_count += consistency\n",
    "                total_comparisons += 1\n",
    "        \n",
    "        # Calculate average consistency score\n",
    "        avg_consistency = consistency_count / total_comparisons if total_comparisons > 0 else 0\n",
    "        num_transitions = len(country_transitions[country]) - 1 if country_transitions[country] else 0\n",
    "        country_consistency_scores.append((country, avg_consistency, num_transitions, years_present))\n",
    "    \n",
    "    # Create DataFrame and display top and bottom countries by consistency\n",
    "    consistency_df = pd.DataFrame(country_consistency_scores, \n",
    "                                columns=['Country', 'Consistency_Score', 'Transitions', 'Years_Present'])\n",
    "    consistency_df = consistency_df.sort_values('Consistency_Score', ascending=False)\n",
    "    \n",
    "    print(\"\\nCountries with the most stable cluster communities:\")\n",
    "    display(consistency_df.head(15))\n",
    "    \n",
    "    print(\"\\nCountries with the least stable cluster communities:\")\n",
    "    display(consistency_df.tail(15))\n",
    "    \n",
    "    return stability_df, consistency_df\n",
    "\n",
    "# Run the enhanced cluster stability analysis\n",
    "community_stability, country_consistency = analyze_cluster_community_stability(data_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68210d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_country_trajectories(data_pca_df, country_list=None, interactive=True):\n",
    "    \"\"\"Visualize how selected countries move through the PCA space over time.\"\"\"\n",
    "    \n",
    "    if country_list is None:\n",
    "        # Use the most interesting countries from consistency analysis\n",
    "        # You can replace this with specific countries of interest\n",
    "        # Assuming country_consistency is available from previous cell\n",
    "        high_consistency = country_consistency.head(5)['Country'].tolist()\n",
    "        low_consistency = country_consistency.tail(5)['Country'].tolist()\n",
    "        country_list = high_consistency + low_consistency\n",
    "    \n",
    "    # Filter for selected countries only\n",
    "    trajectory_df = data_pca_df[data_pca_df['Country Name'].isin(country_list)].copy()\n",
    "    \n",
    "    if interactive:\n",
    "        # Create an interactive plot with Plotly\n",
    "        fig = px.line(trajectory_df, x='PC1', y='PC2', color='Country Name', \n",
    "                     hover_data=['Year', 'Cluster'],\n",
    "                     title='Country Trajectories Through PCA Space')\n",
    "        \n",
    "        # Add markers for each point\n",
    "        fig.update_traces(mode='lines+markers')\n",
    "        \n",
    "        # Improve appearance\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='white',\n",
    "            xaxis=dict(title='Principal Component 1', showgrid=True, gridwidth=0.5, gridcolor='lightgray'),\n",
    "            yaxis=dict(title='Principal Component 2', showgrid=True, gridwidth=0.5, gridcolor='lightgray'),\n",
    "            height=600,\n",
    "            width=900,\n",
    "            legend_title='Country'\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    else:\n",
    "        # Create a static plot with matplotlib/seaborn\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for country in country_list:\n",
    "            country_data = trajectory_df[trajectory_df['Country Name'] == country]\n",
    "            plt.plot('PC1', 'PC2', data=country_data, marker='o', linewidth=1.5, label=country)\n",
    "            \n",
    "        plt.title('Country Trajectories Through PCA Space')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output/plots/country_trajectories.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Create trajectory visualization for a selection of interesting countries\n",
    "# For example, countries with the most stable and least stable cluster memberships\n",
    "visualize_country_trajectories(data_pca_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
