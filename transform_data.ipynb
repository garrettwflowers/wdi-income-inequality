{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f702809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input/raw/WDICSV.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df.melt(id_vars=[\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "df_wide = df_long.pivot_table(index=[\"Country Name\", \"Country Code\", \"Year\"],\n",
    "                              columns=\"Indicator Code\", values=\"Value\")\n",
    "\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "df_wide['Year'] = df_wide['Year'].apply(lambda year: pd.to_datetime(year, format='%Y') + relativedelta(month=12, day=31))\n",
    "\n",
    "df_wide.to_csv('input/transformed/df_wide.csv', index=False)\n",
    "\n",
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target indicators for top and bottom 10% wealth share\n",
    "target_top = \"SI.DST.10TH.10\"    # Top 10% wealth share\n",
    "target_bottom = \"SI.DST.FRST.10\" # Bottom 10% wealth share\n",
    "target_gini = \"SI.POV.GINI\"      # Gini index\n",
    "\n",
    "# Exclude identifier columns and target columns from features\n",
    "exclude_columns = [\"Country Name\", \"Country Code\", \"Year\", target_top, target_bottom, target_gini]\n",
    "wealth_share_columns = [col for col in df_wide.columns if col.startswith(\"SI.DST\") and col not in exclude_columns]\n",
    "exclude_columns.extend(wealth_share_columns)\n",
    "feature_columns = [col for col in df_wide.columns if col not in exclude_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44458a91",
   "metadata": {},
   "source": [
    "## Missing Value Handling with KNN Imputation\n",
    "- took ~25min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbccae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a copy of the feature data\n",
    "X_impute = df_wide[feature_columns].copy()\n",
    "\n",
    "# Check missing values before imputation\n",
    "missing_before = X_impute.isna().sum().sum()\n",
    "print(f\"Missing values before imputation: {missing_before:,}\")\n",
    "\n",
    "# Approach 1: First scale, then impute\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_impute),\n",
    "    columns=X_impute.columns,\n",
    "    index=X_impute.index\n",
    ")\n",
    "\n",
    "# Apply KNN imputation on the scaled data\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed_scaled = pd.DataFrame(\n",
    "    imputer.fit_transform(X_scaled),\n",
    "    columns=X_impute.columns,\n",
    "    index=X_impute.index\n",
    ")\n",
    "\n",
    "# Convert back to original scale if needed\n",
    "X_imputed = pd.DataFrame(\n",
    "    scaler.inverse_transform(X_imputed_scaled),\n",
    "    columns=X_impute.columns,\n",
    "    index=X_impute.index\n",
    ")\n",
    "\n",
    "# Update the dataframe\n",
    "df_wide_knn = df_wide.copy()\n",
    "df_wide_knn[feature_columns] = X_imputed\n",
    "\n",
    "# Save the transformed and imputed data for later use\n",
    "df_wide_knn.to_csv('input/imputed/df_wide_knn_imputed.csv', index=False)\n",
    "\n",
    "# Check missing values after imputation\n",
    "missing_after = df_wide_knn[feature_columns].isna().sum().sum()\n",
    "print(f\"Missing values after imputation: {missing_after:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
